"""程序说明"""
# -*-  coding: utf-8 -*-
# Author: cao wang
# Datetime : 2020
# software: PyCharm
# 收获:
"""寻找数据的有用表示，表述为数据，求出数据表式
数值表示---权重*数值表示---损失函数进行比较---优化器求导进行相减---重复直到损失函数判定通过
深度学习优越性在于特征工程的自动化，寻找自动的数据最好表示？对数据进行重建?
逐层渐进、共同渐进，一次多层，一共多次
深度学习运用于感知问题，因为可以十分细微；梯度提升树xgboost用于结构化问题-----典型的结构化数据包括：信用卡号码、日期、财务金额、电话号码、地址、产品名称等

张量：数值容器，矩阵是二维张量，列表形式
"""
import numpy as np
x = np.array(12)
print(x)
print(x.ndim)#一个数值为标量，一个列表括号为一个张量！！
"""
数据第一个轴一般就是样本数量，即批量
2d:批量+数值
3d：批量+时间+数值
4d：批量+颜色+长+宽
5d：视频=批量+帧数+颜色+长+宽

relu、偏置都是逐个元素计算即遍历二维张量
广播：即张量重复扩大
张量点积是在确认数值基础上计算的，确认第i行j个数值，然后计算乘积
两个张量的行与列相同，才可以进行点积计算

几何解释：张量就是维度空间，机器学习与深度学习就是将维度下降，找出简洁的表示，因而深度学习的层级就是一步步降维的过程？！
梯度：就是导数
反向传播：损失值就是预测与真实值的差距，就是y-y1,那么此处求导就是x-x1,就是导数；；
学习率就是步长就是此处的x与x1的差距，自行设置小了迭代次数就会多陷入局部最小值，大了就会找不到合适的参数；
多个导数如同链条一样求导就是反向传播算法
梯度下降的方法就是优化器：adam等等
"""
#张量变形
x = np.array([[0.,1],
             [2.,3],
             [3.,4]])#表中表为二维张量
print(x.shape)
x = x.reshape((6,1))
print(x)

#转置
x = np.zeros((3,2))
print(x)
x = np.transpose(x)#行列交换
print(x.shape)
print(x)




