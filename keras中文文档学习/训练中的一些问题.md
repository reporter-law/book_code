# loss为负数
原因：导数为负数

解决方法： Relu 的一个优点是：当是负值的时候，导数等于 0

就是增加激活函数，leakyrelu,relu,elu,softmax

#ValueError:  `initial_state` to a Bidirectional RNN
错误：ValueError: When passing `initial_state` to a Bidirectional RNN, the state should be a list containing the states of the underlying RNNs. Found: [<tf.Tensor 'model_1/SOP/Softmax:0' shape=(?, 2) dtype=float32>]

解决方式：albert_model = load_brightmart_albert_zh_checkpoint(model_path, training=False)，不能True

#ELU
不能use_bis=True

#Blas GEMM launch failed 
问题：显存没有释放

解决方法：
```python
import tensorflow as tf
from keras.backend.tensorflow_backend import set_session
config = tf.ConfigProto()
config.gpu_options.allocator_type = 'BFC' #A "Best-fit with coalescing" algorithm, simplified from a version of dlmalloc.
config.gpu_options.per_process_gpu_memory_fraction = 0.3
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
```

#Layer does not support masking
问题：cnn不支持mask

解决方法
```python
x = Lambda(lambda x: x, output_shape=lambda s: s)(x)
x = Conv1D(self.cnn_filters, self.cnn_kernel_size, padding='same', activation='relu')(x)
```

# Invalid argument: Matrix size-incompatible
问题：CRF sparse_target=False, 

方法：使用的label需要做one hot，如果没做就会报这个错。
改成sparse_target=True就使用数字label不需要one hot了。

结论，这个不影响精度，只是格式问题

#Keras AttributeError: 'str' object has no attribute 'decode'
原因：h5py版本过高

方法：降低h5py版本为2.10.0

#规律
第一条规律：如果训练损失没有减少，则该模型对于数据来说太简单了。

第二条规律：如果训练损失和验证损失分道扬镳（diverge）了，则说明模型过拟合了

#隐藏层
由于历史原因，第一个 线性+激活层 通常称为隐藏层，因为它的输出不会直接观察到，而是会送到输出层

#pytorch与keras的不同
在于需要说明数据流动过程，计算方式，在keras中被简化了

# Shape()不一致，但是summary是正确的
问题：time_distributed_1 to have shape (None, 20) but got array with shape (128, 1)

原因：one_hot编码问题,标签只是数字编码，不是one-hot编码

解决方法：categorical_crossentropy变为sparse_categorical_crossentropy

函数的功能都是将数字编码转化成one-hot编码格式，
然后对one-hot编码格式的数据（真实标签值）与预测出的标签值使用交叉熵损失函数。

#'ConcatV2' Op 
问题：Tensors in list passed to 'values' of 'ConcatV2' Op have types [bool, float32] that don't all match.

解决方法：换回keras==2.2.4

#keras-pad_sequences
问题：列表状态下漏了词语，只有最后一个词语，但是要求输入的就是嵌套的列表

解决方式:循环或者将原序列转为np.arrar()